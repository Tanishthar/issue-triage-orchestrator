# Minimal Multi-Agent Orchestrator â€” Issue Triage Workflow

### Owner: Tanish
### Status: âœ… Production Ready - Fully functional with Ollama integration, comprehensive logging, and robust error handling.

## Quick summary

This repository implements a minimal multi-agent orchestrator for an Issue Triage workflow:
- Input: GitHub repo + issue text
- Workflow: **LLM-based** severity classification â†’ repro extraction â†’ fix proposer â†’ failing unit test generation â†’ dry-run PR creation (artifact)
- Stack: FastAPI (orchestrator), Next.js + Tailwind (dashboard), **Ollama LLM agent with tool calling**, typed state with Pydantic, reliability layer (retries, backoff, circuit-breaker), tools (safe HTTP fetcher, mock vector store, executor sandbox), evaluation harness, WebSocket live logs.

## Quickstart (one-command)

1. Copy env (optional, falls back to mock mode if Ollama not running):
```bash
cp .env.example .env
# Optional: Set OLLAMA_BASE_URL if Ollama is not at default location
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# Optional: Set DEFAULT_LLM_MODEL to specify which Ollama model to use
# DEFAULT_LLM_MODEL=ollama:llama3.1
```

**Note:** The system uses Ollama models only. Ensure Ollama is running locally at http://localhost:11434. If Ollama is not available, the system runs in mock mode with heuristic-based responses.

**Model Selection:** You can set the `DEFAULT_LLM_MODEL` environment variable to specify which Ollama model to use by default (e.g., `ollama:llama3.1`, `ollama:llama3.2`, `ollama:mistral`). If not set, it defaults to `ollama:llama3.1`. The model can also be changed via the frontend dropdown or API request.

2. Build & run:
```bash
docker compose up --build
```

Or use the Makefile:
```bash
make up
```

3. Visit:
- Dashboard: http://localhost:3000
- Orchestrator API: http://localhost:8000

## Useful API Endpoints

| Method | Endpoint | Description |
| GET | / | Health check. |
| POST | /start | Start a run (body: {"repo":"<git-url-or-sample>","issue_text":"..."}) |
| GET | /logs?tail=200 | Tail recent logs. |
| GET | /metrics | Latest metrics snapshot. |
| WebSocket | /ws | Live logs & metrics stream. |

## LLM Agent Integration

The orchestrator now uses a **real LLM agent** (Ollama) with tool-driven reasoning:

### Features
- **Ollama integration**: Uses local Ollama models for all LLM operations
- **LLM-based classification**: Uses selected model to classify issue severity (low/medium/high/critical)
- **LLM-based repro extraction**: Extracts reproduction steps from issue text
- **Tool-driven reasoning**: Agent can call tools (HTTP fetcher, vector store, syntax checker) to gather context
- **Fix proposal**: LLM generates fix sketches and failing test code
- **Full observability**: All tool calls and reasoning steps are logged
- **Model switching**: Frontend dropdown to switch between Ollama models on-the-fly

### Supported Models
- **Ollama**: Llama 3.1 (default), Llama 3.2, Mistral, Qwen 2.5, and other Ollama models

### Tool Calling
The LLM agent has access to these tools:
1. **fetch_url**: Fetch content from URLs (READMEs, documentation)
2. **search_documentation**: Search vector store for relevant context
3. **check_code_syntax**: Validate generated Python code syntax

### Safety & Budget
- **Mock mode fallback**: If no API key is set, uses heuristic-based mock responses
- **Token limits**: Max 2000 tokens per LLM call
- **Timeout protection**: 5-minute timeout for API calls
- **Error handling**: Graceful degradation on API failures

### Prompts
The agent uses structured prompts for:
- Severity classification (considers impact, frequency, workarounds, security)
- Repro step extraction (numbered list format)
- Fix proposal (JSON format with fix_sketch and test_code)

See `packages/agents/llm_agent.py` for prompt details.

## Reproducible Sample Runs

### LLM Agent Demo
Run a demonstrable LLM agent workflow:
```bash
python scripts/run_llm_demo.py
```

This demonstrates:
- LLM severity classification
- LLM repro step extraction  
- Tool-driven reasoning (if README available)
- Fix proposal with test generation
- Full artifact capture

### Legacy Sample Runs
Run the provided script to execute 3 sample runs and persist artifacts:
```bash 
scripts/run_sample_runs.sh
```

Artifacts saved under `sample_runs/` and `packages/tools/_dry_prs/`. Metrics written to `metrics/metrics.json`.

## Repo Layout

issue-triage-orchestrator/
â”œâ”€ apps/
â”‚Â  â”œâ”€ orchestrator/         # FastAPI app + orchestrator code
â”‚Â  â””â”€ web/                  # Next.js dashboard
â”œâ”€ packages/
â”‚Â  â”œâ”€ tools/                # safe HTTP fetcher, mock vector store, dry PR generator
â”‚Â  â””â”€ eval/                 # evaluation harness
â”œâ”€ metrics/                  # persisted metrics.json and step_logs.json
â”œâ”€ sample_runs/              # generated by scripts (3 sample runs)
â””â”€ docker-compose.yml


## Evaluation Harness & Rubric Mapping
All agent outputs are subject to validation to ensure quality.
- Schema validation: Outputs validated against typed Pydantic schemas.
- Metrics recorded: tool_error_rate, steps_count, eval_score â€” persisted to metrics/metrics.json.
- Checks included:
 - schema_valid: boolean
 - unique_sources_count >= 1 (for recon flows)
 - pr_artifact_created: boolean
 - failing_unit_test_present: boolean
See packages/eval/harness.py for implementation details.

## Development & Debugging Tips ðŸ’¡
- If FastAPI fails to start, confirm apps/orchestrator/Dockerfile CMD uses correct module path (e.g., apps.orchestrator.main:app).
- To inspect PR artifacts:
```bash
ls packages/tools/_dry_prs/
```
- To view metrics:
```bash 
cat metrics/metrics.json
```

## LLM Integration Details

### Architecture
- **Agent**: `packages/agents/llm_agent.py` - Core LLM agent with tool calling
- **Tool Wrappers**: `packages/agents/tool_wrappers.py` - Wraps tools for LLM use
- **Orchestrator Integration**: `apps/orchestrator/main.py` - Uses LLM agent instead of mocks

### Control Flow
1. **Initialization**: LLM agent created with registered tools
2. **Severity Classification**: Direct LLM call with issue text
3. **README Fetching**: Traditional tool call (not LLM-driven)
4. **Vector Store Indexing**: Index README for context
5. **Repro Extraction**: LLM extracts steps from issue
6. **Tool-Driven Reasoning** (if README available):
   - Agent can search vector store for context
   - Agent can fetch additional URLs if needed
   - Agent validates code syntax
7. **Fix Proposal**: LLM generates fix sketch and test code
8. **Syntax Validation**: Executor validates generated code
9. **PR Creation**: Dry-run PR artifact created

### Logging & Observability
All tool calls are logged with:
- **Tool usage logs**: Each tool logs "Used tool: [tool_name]" before execution
- Tool name and arguments
- Tool results (truncated to 500 chars)
- Tool errors
- LLM reasoning iterations
- Real-time tool call counts
- Full control loop visibility

Check `metrics/step_logs.json` for complete execution trace. All tool invocations are tracked and logged for full observability.

### Budget Considerations
- **Model**: Defaults to `ollama:llama3.1` (local, no API costs)
- **Max tokens**: 2000 per call (prevents runaway processing)
- **Max iterations**: 3-5 for tool reasoning (prevents loops)
- **Caching**: HTTP fetcher uses cache to reduce network calls
- **Mock mode**: No costs when Ollama not running

## Development Journey & Key Fixes

### Migration to Ollama-Only Architecture
The system was originally designed with multi-model support (OpenAI, Gemini, and Ollama). We migrated to **Ollama-only** for:
- **Cost efficiency**: No API costs with local models
- **Privacy**: All processing happens locally
- **Reliability**: No dependency on external API services
- **Simplicity**: Single model provider reduces complexity

**Changes made:**
- Removed all OpenAI and Gemini API integrations
- Removed `openai` and `google-generativeai` dependencies
- Updated frontend to show only Ollama models
- Simplified model detection and initialization logic

### Critical Bug Fixes

#### 1. Numpy Array Serialization Error
**Issue**: `Object of type ndarray is not JSON serializable` errors when vector store tried to persist or serialize data.

**Root Cause**: Vector store was converting lists to numpy arrays when loading from disk, causing JSON serialization failures.

**Solution**:
- Modified vector store to keep vectors as lists in memory (not numpy arrays)
- Convert to numpy arrays only on-the-fly during search computations
- Added `safe_json_serialize()` helper function to handle numpy arrays recursively
- Updated `_persist()` method to ensure all vectors are lists before saving

**Files changed**: `packages/tools/vector_store.py`, `packages/agents/llm_agent.py`

#### 2. Tool Calls Count Always Showing Zero
**Issue**: Tool calls count always displayed as 0 even when tools were being used.

**Root Cause**: Count was being calculated but not properly displayed in logs.

**Solution**:
- Fixed tool call history tracking in `reason_with_tools()` method
- Ensured `tool_call_history` is properly populated and returned
- Added explicit count calculation and logging

**Files changed**: `apps/orchestrator/main.py`, `packages/agents/llm_agent.py`

#### 3. Missing Tool Usage Logs
**Issue**: No clear indication when tools were being called, making debugging difficult.

**Solution**:
- Added "Used tool: [tool_name]" log statements before each tool execution
- Logs added for all 4 tools:
  - `fetch_url` - when fetching README files
  - `search_documentation` - when searching vector store
  - `check_code_syntax` - when validating code syntax
  - Direct tool calls in orchestrator also logged

**Files changed**: `packages/agents/tool_wrappers.py`, `apps/orchestrator/main.py`, `packages/agents/llm_agent.py`

### Environment Variable Support
Added `DEFAULT_LLM_MODEL` environment variable for easy model configuration:
- Set default model globally via `.env` file
- Works across backend (Python) and frontend (Next.js with `NEXT_PUBLIC_` prefix)
- Falls back to `ollama:llama3.1` if not set
- Can still be overridden via frontend dropdown or API request

**Files changed**: `packages/agents/llm_agent.py`, `apps/orchestrator/main.py`, `apps/web/pages/index.tsx`, `scripts/run_llm_demo.py`

### Current Architecture Status
âœ… **Fully Functional**:
- Ollama integration working reliably
- All 4 tools properly logged and tracked
- Real tool call counts displayed
- Environment variable support
- Comprehensive error handling
- Mock mode fallback when Ollama unavailable

## Next Steps / Stretch Goals
- âœ… LLM integration with tool calling
- âœ… Tool-driven reasoning loop
- âœ… Full observability of tool calls
- âœ… Ollama-only architecture
- âœ… Comprehensive tool logging
- âœ… Environment variable configuration
- ðŸ”„ Streaming responses for better UX
- ðŸ”„ Tool result caching (local vector store)
- ðŸ”„ Canary run to detect performance degradation
- ðŸ”„ Additional Ollama model support