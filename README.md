# Minimal Multi-Agent Orchestrator â€” Issue Triage Workflow

### Owner: Tanish
### Status: Completed through Phase 6 (Dockerized). Phase 7 adds docs, sample runs, and design notes.

## Quick summary

This repository implements a minimal multi-agent orchestrator for an Issue Triage workflow:
- Input: GitHub repo + issue text
- Workflow: severity classification â†’ repro extraction â†’ fix proposer â†’ failing unit test generation â†’ dry-run PR creation (artifact)
- Stack: FastAPI (orchestrator), Next.js + Tailwind (dashboard), typed state with Pydantic, reliability layer (retries, backoff, circuit-breaker), tools (safe HTTP fetcher, mock vector store, executor sandbox), evaluation harness, WebSocket live logs.

## Quickstart (one-command)

1. Copy env:
```bash
cp .env.example .env
# fill any API keys if you plan to use them
```

2. Build & run:
```bash
docker compose up --build
```

Or use the Makefile:
```bash
make up
```

3. Visit:
- Dashboard: http://localhost:3000
- Orchestrator API: http://localhost:8000

## Useful API Endpoints

| Method | Endpoint | Description |
| GET | / | Health check. |
| POST | /start | Start a run (body: {"repo":"<git-url-or-sample>","issue_text":"..."}) |
| GET | /logs?tail=200 | Tail recent logs. |
| GET | /metrics | Latest metrics snapshot. |
| WebSocket | /ws | Live logs & metrics stream. |

## Reproducible Sample Runs

Run the provided script to execute 3 sample runs and persist artifacts:
```bash 
scripts/run_sample_runs.sh
```

Artifacts saved under sample_runs/ and packages/tools/_dry_prs/. Metrics written to metrics/metrics.json.

## Repo Layout

issue-triage-orchestrator/
â”œâ”€ apps/
â”‚Â  â”œâ”€ orchestrator/         # FastAPI app + orchestrator code
â”‚Â  â””â”€ web/                  # Next.js dashboard
â”œâ”€ packages/
â”‚Â  â”œâ”€ tools/                # safe HTTP fetcher, mock vector store, dry PR generator
â”‚Â  â””â”€ eval/                 # evaluation harness
â”œâ”€ metrics/                  # persisted metrics.json and step_logs.json
â”œâ”€ sample_runs/              # generated by scripts (3 sample runs)
â””â”€ docker-compose.yml


## Evaluation Harness & Rubric Mapping
All agent outputs are subject to validation to ensure quality.
- Schema validation: Outputs validated against typed Pydantic schemas.
- Metrics recorded: tool_error_rate, steps_count, eval_score â€” persisted to metrics/metrics.json.
- Checks included:
 - schema_valid: boolean
 - unique_sources_count >= 1 (for recon flows)
 - pr_artifact_created: boolean
 - failing_unit_test_present: boolean
See packages/eval/harness.py for implementation details.

## Development & Debugging Tips ðŸ’¡
- If FastAPI fails to start, confirm apps/orchestrator/Dockerfile CMD uses correct module path (e.g., apps.orchestrator.main:app).
- To inspect PR artifacts:
```bash
ls packages/tools/_dry_prs/
```
- To view metrics:
```bash 
cat metrics/metrics.json
```

## Next Steps / Stretch Goals
- LLM swapping for token-cost optimization
- Tool result caching (local vector store)
- Canary run to detect performance degradation